# Day 1 — Interview Prep

The following questions help reinforce the compilation pipeline concepts. Try to answer
without looking at the source, then compare with the sample answers.

## Warm-up

1. **What are the four major phases of turning a C++ source file into an executable?**  
   *Preprocess → Compile → Assemble → Link. Each stage has its own output artifact.*

2. **What is a translation unit?**  
   *The preprocessed source that results from taking one `.cpp` file plus everything it
   includes; it is the unit of compilation that produces an object file.*

3. **Why do we separate declarations into headers and definitions into `.cpp` files?**  
   *Headers expose interfaces for multiple translation units to share, while keeping
   implementations compiled once and linked together.*

## Practical Commands

4. **How do you only run the preprocessor with `g++` and why might that be useful?**  
   *Use `g++ -E file.cpp -o file.ii`. Inspecting the output helps debug macros and include
   paths.*

5. **What is emitted by `g++ -S` vs. `g++ -c`?**  
   *`-S` stops after producing assembly (`.s`). `-c` continues to assemble into an object
   file (`.o`) without linking.*

6. **How can you inspect symbol information inside an object file?**  
   *Tools like `nm`, `objdump -t`, or `readelf -s` reveal defined and undefined symbols,
   which helps diagnose linker errors.*

## Conceptual Follow-ups

7. **What causes a "multiple definition" linker error and how do you resolve it?**  
   *Defining the same symbol in more than one translation unit. Move common code into a
   single `.cpp`, keep only declarations in headers, or mark inline/constexpr when
   duplication is intended.*

8. **When should you use header guards (or `#pragma once`)?**  
   *Always guard headers to prevent double inclusion during preprocessing, avoiding
   duplicate declarations or definitions.*

9. **How do static libraries fit into this pipeline?**  
   *Each `.cpp` still compiles into `.o` files. An archiver like `ar` bundles them into a
   `.a` static library that the linker can treat as another input when building an
   executable.*

10. **What diagnostic flags help you catch issues early in the pipeline?**
    *`-Wall -Wextra -pedantic` catch many compile-time issues; `-Wl,--fatal-warnings`
    elevates linker warnings. Pair them with `-std=c++20` (or whichever standard you use)
    for consistent language features.*

## Advanced Deep Dives

11. **What actually happens during preprocessing?**
    *The preprocessor handles file inclusion, macro expansion, conditional compilation, and
    line control directives before the compiler proper ever tokenizes the source. It also
    strips comments and may insert diagnostic `#line` markers so the compiler can map
    errors back to original files.*

12. **How is preprocessing related to lexical analysis?**
    *They are sequential. Preprocessing creates a single translation unit stream, then the
    compiler performs lexical analysis on that stream to convert characters into tokens.
    Macros and conditional branches are resolved before lexing; this is why the parser and
    semantic analyzer never "see" code that was excluded by `#if` guards.*

13. **Where do parsing, semantic analysis, and intermediate representation (IR) fit in?**
    *After lexing, the parser (often generated by tools like Yacc/Bison) checks grammar
    rules and builds an abstract syntax tree. Semantic analysis validates types, scopes,
    overload resolution, and template instantiation. Only when the AST is semantically
    sound does the compiler lower it into an IR such as LLVM IR or GCC GIMPLE, which later
    stages can optimize.*

14. **How do modern optimizers leverage IR?**
    *IRs normalize language constructs into a handful of operations that are easier to
    analyze. Optimizations such as constant folding, loop-invariant code motion, and dead
    code elimination operate primarily on IR, which can then be re-lowered into target
    assembly with machine-specific passes.*

15. **What role can tools like Lex and Yacc play in this pipeline?**
    *They model the same front-end steps at a smaller scale. Lex builds a tokenizer, Yacc
    builds a parser, and the semantic actions you write mimic semantic analysis. The
    generated C/C++ files become translation units just like handwritten `.cpp` files and
    must pass through preprocessing, compilation, assembly, and linking.*
